### **The spelled-out intro to neural networks and backpropagation: building micrograd**

Backpropagation: orqaga tarqalish

Backpropagation helps neural networks learn by providing a systematic way to improve their predictions through error correction. Here’s how it works:

- When a neural network makes a prediction, it compares the prediction to the actual answer using a loss function, which measures how far off the prediction was[1](https://www.ibm.com/think/topics/backpropagation)[5](https://www.grammarly.com/blog/ai/what-is-backpropagation/)[6](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Decoding-Backpropagation-and-Its-Role-in-Neural-Network-Learning--Vmlldzo3MTY5MzM1).
- The network then calculates how much each part (weight) of the network contributed to the error. This is done by sending the error backward through the network, from the output layer to the input layer, using the chain rule from calculus[1](https://www.ibm.com/think/topics/backpropagation)[5](https://www.grammarly.com/blog/ai/what-is-backpropagation/).
- As the error is sent backward, the network computes gradients-these are signals that tell each weight how it should be adjusted to reduce the error next time[1](https://www.ibm.com/think/topics/backpropagation)[2](https://builtin.com/machine-learning/backpropagation-neural-network)[5](https://www.grammarly.com/blog/ai/what-is-backpropagation/).
- The network then updates its weights in the direction that will decrease the error, using an optimization algorithm like gradient descent[1](https://www.ibm.com/think/topics/backpropagation)[5](https://www.grammarly.com/blog/ai/what-is-backpropagation/)[6](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Decoding-Backpropagation-and-Its-Role-in-Neural-Network-Learning--Vmlldzo3MTY5MzM1).

By repeating this process for many examples, the neural network gradually learns the best set of weights to make more accurate predictions. Backpropagation is crucial because it allows the network to efficiently identify and correct mistakes, enabling it to learn complex patterns and generalize well to new data

arbitary: o'zboshimchalik bilan, zo'ravon

tensor:

- flexible, multi-dimensional array of numbers that can represent simple or very complex data and relationships

Derivative: hosila

derive: hosil qilish, olmoq

------------------------------
segmentation - division into separate parts or sections

benchmark - a standard or point of reference against which things may be compared or assessed

convolution - a thing that is complex and difficult to follow. a function derived from two given functions by integration which expresses how the shape of one is modified by the other.


### **Lecture 2 | Image Classification**

pipeline - quvur liniyasi

notion - tushuncha, fikr, g’oya

obvious - aniq, ravshan

Image classigication: a core task in computer vision

First step to detect cat from image:

1. Detect edges: not very scalable to approach
2. Data driven approach:
    1. collect a dataset of images and labels
    2. use machine learning to train a classifier
    3. evaluate the classifier on new images

First classifier: Nearest Neighbor

Example Dataset: CIFAR10

- 10 classes
- 50000 training images
- 10000 testing images

Distance metric to compare images: L1 distance

Time:

- Train O(1)
- Predict O(N)

K in K-nearest neighbors - need to choose more than a for more smooth result

k, distance is hyperparameters

Cross-Validation: Split data into folds, try each fold as validation and average the results

k-nearest:

- very slow at test time
- distance metrics on pixels are not informative

Linear Classification

Basic block of deep neural networks

W - parameters or weights
-------------------------------------------

### **Lecture 3 | Loss Functions and Optimization**
hinge - oshiq-moshiq
soft constraint - yumshoq cheklov
sparse - siyrak

📷 Image Classification
Image classification is the task of assigning a category label to an image (e.g., “dog” or “airplane”). 
Each image is usually represented as a high-dimensional vector (e.g., pixel values), 
and the goal is to learn a function that maps these vectors to labels.

------------------------
📈 Linear Classifiers
Linear classifiers are a family of models that make predictions based on a linear combination of the input features. 
In mathematical terms, for an input image vector x, the model computes:

𝑓(𝑥)=𝑊𝑥+𝑏f(x)=Wx+b

Where:
W is a weight matrix
b is a bias vector
The output f(x) is a score for each class
The class with the highest score is predicted.
----------------------------
Loss function - tells how good our current classifier is

Datat loss: model predictions should match training data

Regularization: model should be “simple”, so it works on test data

Elastic net is mixture of L1+L2 learning

Difference between L1, and L2?

Softmax Classifier (Multinomial Logistic Regression)?

score = unnormalized log probabilites of the classes

minus log of probability of true class?

![image.png](attachment:9b91bc27-208d-4a18-865b-68932b0e2c1b:image.png)

What is hinge loss?

Slope - derivative of a function

Gradient - vector of (partial derivatives) along each dimension

Analytic gradient?

Always use analytic gradient, but to check we can use numerical gradient

Stochastic Gradient Descent

Feature transform
-------------------------
### **Lecture 4 | Introduction to Neural Networks**

adequate - talabga javob beradigan, yetarli

Gradient - an inclined part of a road or railway; a slope(qiyalik)

**Computational Graphs**

A **computational graph** is a way to visualize and structure the computation performed by a machine learning model, especially in deep learning.

- **Nodes**: Variables or operations (e.g., add, multiply, sigmoid).
- **Edges**: Flow of data between operations.

For example, the expression `z = (x + y) * w` would be broken into smaller operations in the graph:

**Why it matters**: It allows automatic differentiation, where the framework (like PyTorch or TensorFlow) can compute gradients of any variable with respect to another.

---

🔙 **Backpropagation**

**Backpropagation** is the algorithm used to compute gradients **efficiently** through a computational graph.

Steps:

1. **Forward pass**: Compute the output of the network (called **feed-forward computation**).
2. **Backward pass**: Apply the chain rule to compute gradients of the loss with respect to all weights (via the computational graph).

This lets the network **learn** by updating weights using gradient-based optimization.

---

📈 **Feed-Forward Computation**

This is the **first step** in training or inference:

- Inputs pass through layers (e.g., convolution, ReLU, fully connected).
- Each layer applies mathematical operations to the data.
- The result is the output (e.g., class scores in image classification).

In training, this output is compared with the true label to compute a **loss**, which then triggers backpropagation.

---

🔽 **Gradient**

The **gradient** tells us how much a change in input or a parameter (like a weight) affects the output or loss.

Mathematically, for a scalar function f(x)f(x)f(x), the gradient is a vector of partial derivatives:

∇f(x)=[∂f∂x1,∂f∂x2,...,∂f∂xn]\nabla f(x) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n} \right]

∇f(x)=[∂x1∂f,∂x2∂f,...,∂xn∂f]

Used in **gradient descent** to update weights and reduce loss:

W←W−η⋅∇WLW \leftarrow W - \eta \cdot \nabla_W L

W←W−η⋅∇WL

---

🧮 **Jacobian Matrix**

When dealing with vector-valued functions (e.g., output from a layer), the **Jacobian matrix** generalizes the gradient:

Jij=∂yi∂xjJ_{ij} = \frac{\partial y_i}{\partial x_j}

Jij=∂xj∂yi

This matrix describes how each output dimension changes with respect to each input dimension. It’s essential in deep learning for understanding how multi-output systems respond to changes in input, and is used internally in backpropagation.

---

🧠 **AlexNet**

**AlexNet** is a famous convolutional neural network (CNN) that **won the ImageNet 2012 competition** and helped spark the deep learning boom.

Key features:

- First to use **ReLU** activations in CNNs.
- Used **dropout** and **data augmentation** to reduce overfitting.
- Deep architecture with **convolutional and fully connected layers**.

AlexNet was trained using feed-forward computation and backpropagation through a computational graph.

---

🧠💾 **Neural Turing Machine (NTM)**

The **Neural Turing Machine** is a more advanced architecture developed by DeepMind that combines:

- A **neural network controller** (like an RNN)
- A **differentiable memory** matrix

It can **learn algorithms** like copying, sorting, or recalling sequences, which standard networks struggle with.

NTMs use gradients (computed through backpropagation) to learn **how to read from and write to memory**, using soft attention mechanisms.
----------------------------
### **Lecture 5 | Convolutional Neural Networks**

convolve - yig'ilish

spatially - fazoviy

stride - katta qadam tashlash

extent - darajada, miqyos

### 🧠 **Perceptron**

The **perceptron** is the **simplest neural unit**, proposed by Frank Rosenblatt in 1958. It mimics a biological neuron by computing:

y=activation(wTx+b)y = \text{activation}(w^T x + b)

y=activation(wTx+b)

It led to the development of **multi-layer perceptrons (MLPs)**, which are fully connected networks. 
However, MLPs don’t scale well for images due to high dimensionality and no spatial awareness.

---

### 👁️ **Neocognitron**

The **neocognitron**, proposed by Kunihiko Fukushima in 1980, 
was the **first model** to use a **hierarchical structure** with **local receptive fields** (precursors to convolutions).

- Introduced concepts of **convolution** and **pooling** (then called “shift invariance”)
- Inspired modern CNNs
- Could recognize characters and shapes regardless of location

---

### 🧮 **Convolutional Neural Networks (CNNs)**

**CNNs** are specialized neural networks for **image and visual data**. 
nstead of fully connecting all neurons, CNNs take advantage of the **spatial structure** of images.

Key components:

1. **Convolution Layers**output=image∗filter
    
    Apply **filters (kernels)** that slide over the image to detect features like edges, textures, shapes.
    
    output=image∗filter\text{output} = \text{image} * \text{filter}
    
2. **Pooling Layers**
    
    Reduce the spatial size of the feature maps to make the network more efficient and robust.
    
    Common types:
    
    - Max pooling: Takes the max value in each patch
    - Average pooling: Averages the values
3. **Fully-Connected Layers**
    
    At the end of the CNN, high-level features are fed into fully connected layers for **classification**.
    

---

### 🧱 **LeNet**

**LeNet-5**, developed by Yann LeCun in 1998, is one of the **first successful CNNs**, used for digit recognition (e.g., MNIST).

Architecture:

- **Convolution → Pooling → Convolution → Pooling → Fully connected layers**
- Simple but effective structure

LeNet proved that CNNs could learn meaningful visual features without manual engineering.

---

### 🏆 **AlexNet**

**AlexNet** (2012, by Alex Krizhevsky) **revolutionized computer vision** by winning the ImageNet competition with a huge margin.

Key innovations:

- Deeper network than LeNet
- Used **ReLU** activation (faster training than tanh/sigmoid)
- Introduced **dropout** to reduce overfitting
- Trained on GPUs for scalability
- **Convolution + Pooling + Fully connected + Softmax**

AlexNet showed that deep learning + big data + GPU = powerful results.
---------------------
### **Lecture 6 | Training Neural Networks I**

Saturated - To'yingan
Squash - ezmoq pachaqlamoq


What does zero centered mean?
    A distribution is zero-centered if its mean is close to 0. 
    This is important in deep learning for both inputs and activations.
    When inputs or activations are zero-centered, the gradients during backpropagation are more balanced.
    If not zero-centered, gradients might consistently point in a biased direction
    (e.g., always positive), which slows learning.
    🔧 Example:
    Data preprocessing often includes mean subtraction to make input features zero-centered.
    Some activation functions (like tanh) are zero-centered; ReLU is not.

How high learning rate will affect to dead relu?
    A dead ReLU is a neuron that always outputs zero because its input is negative and never activates.
    A high learning rate can make this worse:
    Large weight updates might push the neuron's weights such that its input is always negative.
    Since ReLU returns 0 for negative inputs and has zero gradient, it stops learning—this is "dead."
    The neuron can’t recover unless weights are adjusted by another mechanism (e.g., better initialization or using Leaky ReLU).
    🛠️ Prevention tips:
        Use lower learning rates
        Use Leaky ReLU or ELU
        Use proper weight initialization (e.g., He initialization for ReLU)

What does saturated neuron mean?
    A saturated neuron is a neuron whose activation output is stuck near the extremes of the activation function, 
    where gradients are very small (close to 0).
    Typical with:
        Sigmoid: Saturates near 0 or 1
        Tanh: Saturates near -1 or 1
    🧨 Problem:
        Gradients vanish in saturated regions → very slow learning or no learning at all (vanishing gradient problem).
    🔧 Solutions:
        Use ReLU instead (non-saturating for positive values)
        Use batch normalization
        Avoid large weights or poor initialization

Xavier initialization
    Xavier initialization (also called Glorot initialization) is a method to initialize weights so that the variance of 
    activations remains stable across layers.
    Applies to:
        Activation functions like sigmoid or tanh
        Can also use a normal distribution variant.
    🔍 Why use it?
        Prevents activations from blowing up or vanishing
        Keeps gradient flow stable during training

Gaussian activation
The term Gaussian activation is not standard, but it usually refers to:
Outputs values in (0, 1)
Bell-shaped curve
Rarely used in deep networks, but appears in Radial Basis Function (RBF) networks
2. Gaussian Noise or Distribution in Activations:
Sometimes “Gaussian” is used to describe:
Adding Gaussian noise to activations (a form of regularization)
Distributions of activations or weights being Gaussian (i.e., normally distributed)
🧠 Bottom line:
    Gaussian activation functions are not common in modern deep learning compared to ReLU or tanh,
    but Gaussian-shaped curves appear in RBF networks or certain generative models.

-----------------------------------------
### **Lecture 7 | Training Neural Networks II**

convex - do'ng

decay - parchalanish, yiringlash

stochastisity - tizimda tasodifiylik yoki tasodif mavjudligini bildiradi

pervasive - keng tarqalgan

Difference in Initialization too small, too big, just right

zero mean

unit variance

Fancier optimization

regulerization

Transfer learning

Stochastic gradient decent

saddle point

SGD+Momentum: how does it help to solve the problem?

Nesterov Momentum, difference

Ada Grad

RMSProp

Adam

DropConnect

Fractional Max Pooling

Batch normalization

Transfer Learning with CNNs

Keywords: Optimization, momentum, Nesterov momentum, AdaGrad, RMSProp, Adam, 
second-order optimization, L-BFGS, ensembles, regularization, dropout, data augmentation, 
transfer learning, finetuning
------------------------------------

### **Lecture 8 | Deep Learning Software**

placeholder - a significant zero in the decimal representation of a number.

batching - to'plash

fused operations - birlashtirilgan operatsiyalar

AutoGrad in Pytorch
    🔁 **AutoGrad in PyTorch**
    **Autograd** is PyTorch’s **automatic differentiation engine**. It records operations on tensors in a 
    **computational graph**, so it can automatically compute gradients for backpropagation.
    How it works:
    1. You perform operations on `torch.Tensor` objects with `requires_grad=True`.
    2. PyTorch builds a **dynamic computational graph** on the fly.
    3. When you call `.backward()` on a scalar loss tensor, PyTorch computes the gradients of all tensors that 
    contributed to that value.
    4. Gradients are stored in the `.grad` attribute.

What is wrapper in Tensorflow and Pytorch?
    A **wrapper** is a utility or layer that **simplifies or extends functionality**. In deep learning frameworks, 
    wrappers are often used to:
    **Abstract low-level APIs** into high-level user-friendly functions
    **Add additional features** (like monitoring, logging, or preprocessing)
    Wrap **native Python functions or classes** to be compatible with the framework

    📦 Examples:
    **In PyTorch**:
    - `torch.nn.Module` is often subclassed or wrapped to make model definitions modular.
    - Custom loss functions or layers can be written by wrapping existing components.
    **In TensorFlow**:
    - `tf.keras.layers.Wrapper` is a base class for things like `TimeDistributed` and `Bidirectional`, which **wrap 
    other layers** to modify their behavior.
    - You can also wrap functions with `@tf.function` to compile them into TensorFlow static graphs.

Batch and minibatch?
    🧠 What is a Batch?
    A batch is a group of training samples (examples) processed together before the model updates its weights.
    Instead of updating weights after every single input (which would be slow and noisy), we collect multiple inputs, 
    run them through the model together, compute the average loss, and then update the weights.

    🧠 What is a Mini-Batch?
    In most real-world scenarios, when we say “batch,” we actually mean mini-batch.
    A mini-batch is a small subset of the full dataset, used in one training iteration. It’s a balance between:
    Stochastic Gradient Descent (SGD) – updates weights after every single sample (very noisy).
    Batch Gradient Descent – updates weights after computing gradients on the entire dataset (very slow for large data).
    Mini-batch Gradient Descent:
    Processes a small number of samples per update (e.g., 16, 32, 64)
    Most common method in deep learning

Static vs Dynamic Graphs
    ⚙️ **Static Computation Graphs**
    (**Define-and-run**)
    Used by **TensorFlow 1.x**, **Theano**, **MXNet** (default mode)
    You first **define the entire graph** (like a blueprint).
    Then you **compile** and **run** it separately with data.
    ✅ Pros:
    Can be **optimized** before execution (graph-level optimizations).
    Often faster in **inference and training** due to those optimizations.
    Easy to **deploy** to production.
    ❌ Cons:
    Harder to debug (you don’t see values during graph building).

    🔄 **Dynamic Computation Graphs**
    (**Define-by-run**)
    Used by **PyTorch**, **TensorFlow 2.x (by default)**, **Chainer**
    The graph is **built on the fly** as you run your code.
    Each operation adds a new node to the graph during execution.
    ✅ Pros:
    Much easier to **debug** using standard Python tools (like `print` or `pdb`).
    **Flexible**: You can use loops, `if` statements, recursion—just like normal Python.
    Better for **research and prototyping**.
    ❌ Cons:
    Slightly slower than static graphs in some cases (though improving fast).
    Harder to optimize globally, since the graph isn’t known ahead of time.
------------------------

### Lecture 9 | CNN Architectures

inception - boshlanishi

redundancy - ortiqchalik

residual connections - qoldiq ulanishlar

shallower model - sayoz model

What is pooling layer in alexnet?

What is effective receptive field?

Inception module?

1x1 bottleneck layers in GoogleNet?

What is Stem Network in GoogleNet?

Difference between FC and Conv layers?

Why do we call vanilla convolutional layers?

Vanishing gradients?
---------------------

### **Lecture 10 | Recurrent Neural Networks**

recurrent

glimpses

explicit
---------------------

### **Lecture 11 | Detection and Segmentation**

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, 
multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, 
YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN
-----------------------------

### **Lecture 12 | Visualizing and Understanding**

arbitrary

jitter image

salency maps

1. What is Gradient Ascent?
2. What is fooling image?
3. Gram Matrix in Texture Synthesis?
4. Covarience matrix

Keywords: Visualization, t-SNE, saliency maps, class visualizations, fooling images, feature inversion, DeepDream, 
style transfer
---------------------

### **Lecture 13 | Generative Models**

Keywords: Generative models, PixelRNN, PixelCNN, autoencoder, variational autoencoder, 
VAE, generative adversarial network, GAN
-------------------
### **Lecture 14 | Deep Reinforcement Learning**

Keywords: Reinforcement learning, RL, Markov decision process, MDP, Q-Learning, 
policy gradients, REINFORCE, actor-critic, Atari games, AlphaGo